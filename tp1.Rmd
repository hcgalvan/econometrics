---
title: "Trabajo Practico Nro 1 - Econometrics Curse, 2024"
output: html_notebook
---

## Equipo:

-   Galván, Hugo César

-   García, José Manuel

#### 1. Supponga que desea estimar el siguiente modelo de regresión lineal simple

Y = β0 + β1X + ε

Genere dos variables aleatorias del modelo (Y, X) teniendo en cuenta que el tamaño de muestra sea de 200 y para todos los casos. Tips (elija la media, desvio estandar que desee. Use distribución normal ).

```{r}
## Generación de muestras X e Y
set.seed(123) 
# Definir parámetros
n <- 200 # cant observ
μX <- 10 # media de x
σX <- 2 # desv std x
μY <- 15 # media de y
σY <- 3 # desv std y

# Generar random variables dependientes
Z1 <- rnorm(n)
Z2 <- rnorm(n)

x <- μX + σX * Z1
y <- μY + σY * Z2

dat <- cbind(x,y) #junto los valores en una lista

```

#### 2. Dibuje un diagrama de dispersión de las variabes simuladas. ¿Qué observa?.¿ Alguna relación lineal se observa?

```{r}
library(ggplot2)

pairs(dat) # otra forma de verificar relación, una matriz de diagramas de dispersión
plot(dat)

ggplot(dat, aes(x = x, y = y)) +
  geom_point()
```

Se observa una baja o nula relación entre las variables x e y. No se percibe un patrón lineal entre ambas variables.

#### 3. Realice una regresión lineal convencional y muestre sus resultados. Interprete la significatividad individual y grupal del modelo.

```{r}
# Realizo regresión lineal 
regresion <- lm(y ~ x, data = data.frame(dat))
summary(regresion)
#boxplot(regresion$residuals)
```

En modelos lineales simples, dado que solo hay un predictor, el *p-value* del test F es igual al *p-value* del *t-test* del predictor.

Significatividad individual:

-   La primera columna (*Estimate*) devuelve el valor estimado para los dos parámetros de la ecuación del modelo lineal (b0 y b1) que equivalen a la ordenada en el origen y la pendiente.

-   Se muestran los errores estándar, el valor del estadístico *t* y el *p-value* (dos colas) de cada uno de los dos parámetros. Esto permite determinar si los parámetros son significativamente distintos de 0, es decir, que tengan importancia en el modelo. En los modelos de regresión lineal simple, el parámetro más informativo suele ser la pendiente.

-   Para el modelo generado, la ordenada en el origen (intercept) es significativa, y la pendiente b1 no es significativa (*p-values* \> 0.1).

Significatividad Grupal del model

-   El valor de r2 indica que el modelo calculado explica el -0.004% de la variabilidad presente en la variable respuesta (*y*) mediante la variable independiente (*x*).

-   El *p-value* obtenido en el test F (0.697) determina que no es significativamente superior la varianza explicada por el modelo en comparación a la varianza total. Es el parámetro que determina si el modelo es significativo y por lo tanto no es aceptable.

#### 4. Agregue la linea de regresión al diagrama de dispersión que graficó en el punto 1.¿ qué observa?

```{r}
library(psych)
library(GGally)
ggpairs(dat, aes(alpha = 0.5),lower = list(continuous = "smooth"))

# Creamos el gráfico
plot(x, y)
# Línea de regresión
abline(lm(y ~ x))



```

Se observa una linea prácticamente horizontal y una dispersión de puntos que se alejan de la recta.

#### 5. En caso de que no observe relación entre las variables ¿Como podríaa solucionarlo? Tips (puede resolver modificando el punto 1 y repitiendo todos los puntos , 2, 3 y 4)

```{r}
set.seed(123)  # Set a seed for reproducibility

# Definir parámetros
n <- 200 # cant observ
μX <- 10
σX <- 2
μY <- 15
σY <- 3
ρ <- 0.7 #grado de correlación

# Generar random variables dependientes
Z1 <- rnorm(n)
Z2 <- rnorm(n)

x <- μX + σX * Z1
y <- μY + σY * (ρ * x + sqrt(1 - ρ^2) * Z2)
dat <- cbind(x,y) #junto los valores en una lista
plot(dat)
```

```{r}
ggpairs(dat, aes(alpha = 0.5),lower = list(continuous = "smooth"))
```

```{r}
# Realizo regresión lineal 
modelo <- lm(y ~ x, data = data.frame(dat))
summary(modelo)
```

#### 6. En base al modelo de regresión realice una predicción para 20 datos más.

```{r}
set.seed(123)
Z1 <- rnorm(20)
x <- μX + σX * Z1
predict(object=modelo, newdata=data.frame(x))
```

#### 7. Simule mediante Montecarlo 1000 regresiones lineales tomando de base el modelo de la ecuación (1). TIPS: en un primer paso coloque valores numéricos de los coeficientes a estimar (intercepto , y beta) además un valor para el desvío estándar del error

```{r}
set.seed(123)
# Establecer numero de simulaciones
num_simulations <- 1000
n <- 100
# Establecer parametros del modelo
intercept <- 4.9
beta <- 1.02
error_std_dev <- 2.138

# Inicializar vectores que contendrán los coeficientes estimados
pendientes <- vector(length = num_simulations)
intercepts <- vector(length = num_simulations)

# Perform the simulations
for (i in 1:num_simulations) {
  # Generate the independent variable (x)
  Z1 <- rnorm(n)
  x <- μX + σX * Z1
  
  # Generate the dependent variable (y) using the model equation
  y <- intercept + beta * x + rnorm(n, 0, error_std_dev)
  #μY + σY * (ρ * Z1 + sqrt(1 - ρ^2) * Z2)
  
  # Fit the linear regression model
  model <- lm(y ~ x)
  pendientes[i] <- model$coefficients[2]
  intercepts[i] <- model$coefficients[1]
}


```

#### 8. Realice un histograma para las estimaciones de la pendiente y del intercepto. Indique lo que observa.

```{r}
par(mfrow = c(1, 2))
hist(pendientes, main = "Distribución de Pendiente",xlab = "Pendiente")
abline(v = mean(pendientes), col="red", lwd=3, lty=2)
hist(intercepts, main = "Distribución de Intercept", xlab = "Intercept")
abline(v = mean(intercepts), col="red", lwd=3, lty=2)

```

#### 9. Realice un gráfico de densidad para las estimaciones de la pendiente y del intercepto. Indique lo que observa.

```{r}
par(mfrow = c(1, 2))
plot(density(pendientes))
plot(density(intercepts))
```

#### 10. Repita la simulación de montecarlo pero sin que la variable X tenga una distribución normal, por ejemplo una distribución uniforme. Considere los puntos 7, 8, 9 de este TP.

```{r}
# Establecer numero de simulaciones
set.seed(123)
num_simulations <- 1000
n <- 100
# Establecer parametros del modelo
intercept <- 4.9
beta <- 2
error_std_dev <- 1

# Inicializar vectores que contendrán los coeficientes estimados
pendientes <- vector(length = num_simulations)
intercepts <- vector(length = num_simulations)

# Perform the simulations
for (i in 1:num_simulations) {
  # Generate the independent variable (x)
  x <- runif(n)
  # plot(x)
  
  # Generate the dependent variable (y) using the model equation
  y <- intercept + beta * x + rnorm(n, 0, error_std_dev)
  
  # Fit the linear regression model
  model <- lm(y ~ x)
  pendientes[i] <- model$coefficients[2]
  intercepts[i] <- model$coefficients[1]
}
par(mfrow = c(2, 2))
hist(pendientes, main = "Distribución de Pendiente Estimadas",xlab = "Pendiente")
hist(intercepts, main = "Distribución de Intercept Estimados", xlab = "Intercept")
plot(density(pendientes))
plot(density(intercepts))
```
